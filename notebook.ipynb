{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6035506a",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db0bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from lightly.models import utils\n",
    "from lightly.models.modules import MAEDecoderTIMM, MaskedVisionTransformerTIMM\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd4d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(seed=3, workers=True)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    \"Precision bf16-mixed is not supported by the model summary\",\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    \"Please use the new API settings to control TF32 behavior\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2cd6a7",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024d3d5f",
   "metadata": {},
   "source": [
    "## Masked Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d74674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAutoencoder(nn.Module):\n",
    "    \"\"\"Masked Autoencoder (MAE) with ViT backbone.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        general_cfg: Dict[str, Any],\n",
    "        encoder_cfg: Dict[str, Any],\n",
    "        decoder_cfg: Dict[str, Any],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mask_ratio = general_cfg.get(\"mask_ratio\", 0.75)\n",
    "        self.image_size = general_cfg.get(\"image_size\", 96)\n",
    "        self.patch_size = general_cfg.get(\"patch_size\", 6)\n",
    "        self.in_chans = general_cfg.get(\"in_chans\", 3)\n",
    "\n",
    "        vit = VisionTransformer(\n",
    "            img_size=self.image_size,\n",
    "            patch_size=self.patch_size,\n",
    "            in_chans=self.in_chans,\n",
    "            embed_dim=encoder_cfg.get(\"embed_dim\", 384),\n",
    "            depth=encoder_cfg.get(\"depth\", 12),\n",
    "            num_heads=encoder_cfg.get(\"num_heads\", 6),\n",
    "            num_classes=0,\n",
    "        )\n",
    "\n",
    "        self.encoder = MaskedVisionTransformerTIMM(vit=vit)\n",
    "        self.sequence_length = getattr(\n",
    "            self.encoder,\n",
    "            \"sequence_length\",\n",
    "            vit.patch_embed.num_patches + 1,\n",
    "        )\n",
    "\n",
    "        self.decoder = MAEDecoderTIMM(\n",
    "            num_patches=vit.patch_embed.num_patches,\n",
    "            patch_size=self.patch_size,\n",
    "            embed_dim=encoder_cfg.get(\"embed_dim\", 384),\n",
    "            decoder_embed_dim=decoder_cfg.get(\"decoder_embed_dim\", 512),\n",
    "            decoder_depth=decoder_cfg.get(\"decoder_depth\", 4),\n",
    "            decoder_num_heads=decoder_cfg.get(\"decoder_num_heads\", 6),\n",
    "        )\n",
    "\n",
    "    def forward_encoder(self, images: torch.Tensor, idx_keep=None):\n",
    "        return self.encoder.encode(images=images, idx_keep=idx_keep)\n",
    "\n",
    "    def forward_decoder(self, x_encoded, idx_keep, idx_mask):\n",
    "        batch_size = x_encoded.shape[0]\n",
    "        x_decode = self.decoder.embed(x_encoded)\n",
    "\n",
    "        x_masked = utils.repeat_token(\n",
    "            token=self.decoder.mask_token,\n",
    "            size=(batch_size, self.sequence_length),\n",
    "        )\n",
    "        x_masked = utils.set_at_index(\n",
    "            tokens=x_masked,\n",
    "            index=idx_keep,\n",
    "            value=x_decode.type_as(x_masked),\n",
    "        )\n",
    "\n",
    "        x_decoded = self.decoder.decode(x_masked)\n",
    "        x_pred = utils.get_at_index(tokens=x_decoded, index=idx_mask)\n",
    "        x_pred = self.decoder.predict(x_pred)\n",
    "\n",
    "        return x_pred\n",
    "\n",
    "    def forward(self, images: torch.Tensor):\n",
    "        batch_size = images.shape[0]\n",
    "        idx_keep, idx_mask = utils.random_token_mask(\n",
    "            size=(batch_size, self.sequence_length),\n",
    "            mask_ratio=self.mask_ratio,\n",
    "            device=images.device,\n",
    "        )\n",
    "\n",
    "        x_encoded = self.forward_encoder(images=images, idx_keep=idx_keep)\n",
    "        x_pred = self.forward_decoder(\n",
    "            x_encoded=x_encoded, idx_keep=idx_keep, idx_mask=idx_mask\n",
    "        )\n",
    "\n",
    "        patches = utils.patchify(images=images, patch_size=self.patch_size)\n",
    "        idx_mask_adj = torch.clamp(idx_mask - 1, min=0)\n",
    "        target = utils.get_at_index(tokens=patches, index=idx_mask_adj)\n",
    "\n",
    "        return x_pred, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394ac95d",
   "metadata": {},
   "source": [
    "## Vision Transformer Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edfd471",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTClassifier(nn.Module):\n",
    "    \"\"\"Classifier built on top of a pretrained ViT encoder.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_encoder: VisionTransformer,\n",
    "        num_classes: int = 10,\n",
    "        head_cfg: Optional[Dict[str, Any]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = pretrained_encoder\n",
    "        head_cfg = head_cfg or {}\n",
    "\n",
    "        embed_dim = head_cfg.get(\"embed_dim\", pretrained_encoder.embed_dim)\n",
    "        pool_type = head_cfg.get(\"pool\", \"cls\")  # or \"mean\"\n",
    "\n",
    "        self.pool_type = pool_type\n",
    "        self.head = torch.nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        feats = self.encoder.forward_features(x)\n",
    "        if isinstance(feats, (tuple, list)):\n",
    "            feats = feats[0]\n",
    "\n",
    "        if self.pool_type == \"cls\":\n",
    "            pooled = feats[:, 0]\n",
    "        else:\n",
    "            pooled = feats.mean(dim=1)\n",
    "\n",
    "        return self.head(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc6196e",
   "metadata": {},
   "source": [
    "# Training Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f63353d",
   "metadata": {},
   "source": [
    "## Masked Autoencoder Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18665d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAEPretrainModule(pl.LightningModule):\n",
    "    \"\"\"Self-supervised pretraining for Masked Autoencoder.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_cfg: Dict[str, Any],\n",
    "        training_cfg: Dict[str, Any],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = MaskedAutoencoder(\n",
    "            general_cfg=model_cfg[\"general\"],\n",
    "            encoder_cfg=model_cfg[\"encoder\"],\n",
    "            decoder_cfg=model_cfg[\"decoder\"],\n",
    "        )\n",
    "\n",
    "        self.mask_start = training_cfg.get(\"mask_ratio_start\", 0.5)\n",
    "        self.mask_end = training_cfg.get(\"mask_ratio_end\", 0.85)\n",
    "        self.ramp_epochs = training_cfg.get(\"mask_ramp_epochs\", 200)\n",
    "\n",
    "        self.lr = float(training_cfg.get(\"base_learning_rate\", 1.5e-4))\n",
    "        self.weight_decay = float(training_cfg.get(\"weight_decay\", 0.05))\n",
    "        self.warmup_epochs = int(training_cfg.get(\"warmup_epochs\", 20))\n",
    "        self.total_epochs = int(training_cfg.get(\"total_epochs\", 200))\n",
    "        self.batch_size = int(training_cfg.get(\"batch_size\", 512))\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, _ = batch\n",
    "        preds, targets = self(imgs)\n",
    "        loss = self.criterion(preds, targets)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, _ = batch\n",
    "        preds, targets = self(imgs)\n",
    "        loss = self.criterion(preds, targets)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        effective_lr = self.lr * self.batch_size / 256\n",
    "        optimizer = AdamW(\n",
    "            self.parameters(),\n",
    "            lr=effective_lr,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "        def lr_lambda(epoch):\n",
    "            warmup = (epoch + 1) / max(1, self.warmup_epochs)\n",
    "            cosine = 0.5 * (1 + math.cos(math.pi * epoch / self.total_epochs))\n",
    "            return min(warmup, 1.0) * cosine\n",
    "\n",
    "        scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"epoch\", \"name\": \"lr\"},\n",
    "        }\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        \"\"\"Update mask ratio linearly over epochs.\"\"\"\n",
    "        progress = min(self.current_epoch / max(1, self.ramp_epochs - 1), 1.0)\n",
    "        new_mask = self.mask_start + progress * (self.mask_end - self.mask_start)\n",
    "        self.model.mask_ratio = new_mask\n",
    "        self.log(\"mask_ratio\", new_mask, prog_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c2592",
   "metadata": {},
   "source": [
    "## Vision Transformer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d8de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTClassifierTrainModule(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Supervised training for ViTClassifier.\n",
    "\n",
    "    Supports randomly initialized or pretrained ViT encoder.\n",
    "    Provides both fine-tuning and linear probing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_encoder: Optional[torch.nn.Module] = None,\n",
    "        model_cfg: Optional[Dict[str, Any]] = None,\n",
    "        training_cfg: Optional[Dict[str, Any]] = None,\n",
    "        num_classes: int = 10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"pretrained_encoder\"])\n",
    "\n",
    "        self.model_cfg = model_cfg or {}\n",
    "        self.training_cfg = training_cfg or {}\n",
    "\n",
    "        self.learning_rate = float(training_cfg.get(\"learning_rate\", 3e-4))\n",
    "        self.weight_decay = float(training_cfg.get(\"weight_decay\", 0.05))\n",
    "        self.warmup_epochs = int(training_cfg.get(\"warmup_epochs\", 5))\n",
    "        self.total_epochs = int(training_cfg.get(\"total_epochs\", 100))\n",
    "        self.freeze_encoder_flag = self.training_cfg.get(\"freeze_encoder\", True)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Build model\n",
    "        encoder_cfg = self.model_cfg.get(\"encoder\", {})\n",
    "        encoder = (\n",
    "            pretrained_encoder\n",
    "            if pretrained_encoder is not None\n",
    "            else VisionTransformer(\n",
    "                img_size=self.model_cfg[\"general\"][\"image_size\"],\n",
    "                patch_size=self.model_cfg[\"general\"][\"patch_size\"],\n",
    "                in_chans=self.model_cfg[\"general\"][\"in_chans\"],\n",
    "                embed_dim=encoder_cfg.get(\"embed_dim\", 384),\n",
    "                depth=encoder_cfg.get(\"depth\", 12),\n",
    "                num_heads=encoder_cfg.get(\"num_heads\", 6),\n",
    "                num_classes=0,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.model = ViTClassifier(\n",
    "            pretrained_encoder=encoder,\n",
    "            num_classes=self.num_classes,\n",
    "            head_cfg=self.model_cfg.get(\"head\", {}),\n",
    "        )\n",
    "\n",
    "        # Freeze or unfreeze encoder as requested\n",
    "        if self.freeze_encoder_flag:\n",
    "            self.freeze_encoder()\n",
    "        else:\n",
    "            self.unfreeze_encoder()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        logits = self(imgs)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        acc = (logits.argmax(dim=1) == labels).float().mean()\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        logits = self(imgs)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        acc = (logits.argmax(dim=1) == labels).float().mean()\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        logits = self(imgs)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        acc = (logits.argmax(dim=1) == labels).float().mean()\n",
    "\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.model.parameters()),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "        def lr_lambda(epoch):\n",
    "            warmup = (epoch + 1) / max(1, self.warmup_epochs)\n",
    "            cosine = 0.5 * (1 + math.cos(math.pi * epoch / self.total_epochs))\n",
    "            return min(warmup, 1.0) * cosine\n",
    "\n",
    "        scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"monitor\": \"val_loss\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def freeze_encoder(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if \"head\" not in name:\n",
    "                param.requires_grad = False\n",
    "        print(\"ðŸ§Š Encoder frozen (only classifier head is trainable).\")\n",
    "\n",
    "    def unfreeze_encoder(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"ðŸ”¥ Encoder unfrozen (all parameters trainable).\")\n",
    "\n",
    "    def unfreeze_last_layers(self, n_layers: int):\n",
    "        \"\"\"\n",
    "        Unfreezes only the last `n_layers` Transformer blocks of the ViT encoder.\n",
    "        All earlier layers remain frozen.\n",
    "        \"\"\"\n",
    "        encoder = self.model.encoder  # timm VisionTransformer\n",
    "        blocks = encoder.blocks  # list of Transformer blocks\n",
    "        total = len(blocks)\n",
    "\n",
    "        if n_layers < 0 or n_layers > total:\n",
    "            raise ValueError(f\"n_layers must be between 0 and {total}, got {n_layers}\")\n",
    "\n",
    "        print(f\"ðŸ”“ Unfreezing last {n_layers} of {total} encoder layers...\")\n",
    "\n",
    "        # 1) Freeze ALL parameters first\n",
    "        for param in encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # 2) Unfreeze the last N Transformer blocks\n",
    "        for block in blocks[total - n_layers :]:\n",
    "            for param in block.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # 3) Also unfreeze the final LN (norm) layer\n",
    "        if hasattr(encoder, \"norm\"):\n",
    "            for param in encoder.norm.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # 4) Head (classifier) is always trainable\n",
    "        for param in self.model.head.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        print(\"ðŸ”¥ Selective unfreezing complete.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
