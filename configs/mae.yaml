# configs/mae.yaml
# Default configuration for MAE pretraining + ViT architecture

model:
  image_size: 96
  patch_size: 8
  emb_dim: 192
  encoder_layer: 12
  encoder_head: 3
  decoder_layer: 4
  decoder_head: 3
  mask_ratio: 0.75

training:
  seed: 73
  total_epochs: 20
  warmup_epochs: 2
  batch_size: 512
  max_device_batch_size: 512
  base_learning_rate: 1.5e-4
  weight_decay: 0.05
  data_fraction: 0.05

logging:
  output_dir: outputs/pretrain/mae_100
  model_path: vit-mae.pt
  num_workers: 4
